{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe26b007",
   "metadata": {},
   "source": [
    "### How tokenizing text, sentence, words work \n",
    "<img src=\"https://user-images.githubusercontent.com/32620288/166104650-bca608ed-afc3-4c56-8bf2-eebf0b52b054.png\" width=\"400\" height=\"1\">\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ed609",
   "metadata": {},
   "source": [
    "Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n",
    "\n",
    "#### Key points of the article –\n",
    "\n",
    "* Text into sentences tokenization\n",
    "* Sentences into words tokenization\n",
    "* Sentences using regular expressions tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207e29d",
   "metadata": {},
   "source": [
    "##### 1. Sentence Tokenization – Splitting sentences in the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac690c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['British Foreign Secretary Liz Truss, on Thursday, appealed to the international allies to invoke stringent sanctions on Russia until it completely withdraws its forces from Ukraine.',\n",
       " 'Addressing the G7 leaders in Germany, she said that Russian President Vladimir Putin has been humiliating himself on the world stage and urged her counterparts to invoke further sanctions until Moscow agrees on peace.',\n",
       " 'Truss also called her fellow G7 foreign ministers for financial and technical assistance to rebuild Ukraine.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "  \n",
    "text = \"British Foreign Secretary Liz Truss, on Thursday, appealed to the international allies to invoke stringent sanctions on Russia until it completely withdraws its forces from Ukraine. Addressing the G7 leaders in Germany, she said that Russian President Vladimir Putin has been humiliating himself on the world stage and urged her counterparts to invoke further sanctions until Moscow agrees on peace. Truss also called her fellow G7 foreign ministers for financial and technical assistance to rebuild Ukraine.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d8c4d",
   "metadata": {},
   "source": [
    "How sent_tokenize works ?\n",
    "\n",
    "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607a745",
   "metadata": {},
   "source": [
    "##### 2. PunktSentenceTokenizer – When we have huge chunks of data then it is efficient to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e81774e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\divak\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizers\\\\punkt\\\\PY3\\\\PY3\\\\english.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19460/1865728923.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Loading PunktSentenceTokenizer using English pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/PY3/english.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    535\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mGzipFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m                         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl2pathname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\divak\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizers\\\\punkt\\\\PY3\\\\PY3\\\\english.pickle'"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "  \n",
    "# Loading PunktSentenceTokenizer using English pickle file\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "  \n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec365b",
   "metadata": {},
   "source": [
    "##### 3. Tokenize sentence of different language – One can also tokenize sentence from different languages using different pickle file other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6876a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "  \n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
    "  \n",
    "text = 'Hola amigo. Estoy bien.'\n",
    "spanish_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4a4b5",
   "metadata": {},
   "source": [
    "##### 4. Word Tokenization – Splitting words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16b86a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prime',\n",
       " 'Minister',\n",
       " 'Narendra',\n",
       " 'Modi',\n",
       " 'on',\n",
       " 'Friday',\n",
       " ',',\n",
       " 'May',\n",
       " '13',\n",
       " 'expressed',\n",
       " 'condolences',\n",
       " 'over',\n",
       " 'the',\n",
       " 'sudden',\n",
       " 'demise',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'Arab',\n",
       " 'Emirates',\n",
       " 'President',\n",
       " 'Sheikh',\n",
       " 'Khalifa',\n",
       " 'bin',\n",
       " 'Zayed',\n",
       " 'Al',\n",
       " 'Nahyan',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'deeply',\n",
       " 'saddened',\n",
       " 'to',\n",
       " 'know',\n",
       " 'about',\n",
       " 'the',\n",
       " 'passing',\n",
       " 'away',\n",
       " 'of',\n",
       " 'HH',\n",
       " 'Sheikh',\n",
       " 'Khalifa',\n",
       " 'bin',\n",
       " 'Zayed',\n",
       " ',',\n",
       " 'Prime',\n",
       " 'Minister',\n",
       " 'wrote',\n",
       " 'in',\n",
       " 'a',\n",
       " 'twitter',\n",
       " 'post',\n",
       " '.',\n",
       " 'He',\n",
       " 'reminisced',\n",
       " 'that',\n",
       " 'the',\n",
       " 'late',\n",
       " 'UAE',\n",
       " 'president',\n",
       " 'was',\n",
       " 'a',\n",
       " 'great',\n",
       " 'statesman',\n",
       " 'and',\n",
       " 'visionary',\n",
       " 'leader',\n",
       " 'under',\n",
       " 'whom',\n",
       " 'India-UAE',\n",
       " 'relations',\n",
       " 'prospered',\n",
       " '.',\n",
       " 'PM',\n",
       " 'Modi',\n",
       " 'sent',\n",
       " 'heartfelt',\n",
       " 'condolences',\n",
       " 'to',\n",
       " 'UAE',\n",
       " 'from',\n",
       " 'the',\n",
       " 'people',\n",
       " 'of',\n",
       " 'India',\n",
       " ',',\n",
       " 'saying',\n",
       " 'that',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'communities',\n",
       " 'were',\n",
       " 'with',\n",
       " 'the',\n",
       " 'people',\n",
       " 'of',\n",
       " 'UAE',\n",
       " 'in',\n",
       " 'these',\n",
       " 'tough',\n",
       " 'times',\n",
       " '.',\n",
       " 'May',\n",
       " 'his',\n",
       " 'soul',\n",
       " 'rest',\n",
       " 'in',\n",
       " 'peace',\n",
       " ',',\n",
       " 'said',\n",
       " 'Prime',\n",
       " 'Minister',\n",
       " 'Modi',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "text = \"Prime Minister Narendra Modi on Friday, May 13 expressed condolences over the sudden demise of the United Arab Emirates President Sheikh Khalifa bin Zayed Al Nahyan. I am deeply saddened to know about the passing away of HH Sheikh Khalifa bin Zayed, Prime Minister wrote in a twitter post. He reminisced that the late UAE president was a great statesman and visionary leader under whom India-UAE relations prospered. PM Modi sent heartfelt condolences to UAE from the people of India, saying that the Indian communities were with the people of UAE in these tough times. May his soul rest in peace,said Prime Minister Modi.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abd8a6",
   "metadata": {},
   "source": [
    "How word_tokenize works?\n",
    "\n",
    "word_tokenize() function is a wrapper function that calls tokenize() on an instance of the TreebankWordTokenizer class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c6b50",
   "metadata": {},
   "source": [
    "###### 5. Using TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b0e4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prime',\n",
       " 'Minister',\n",
       " 'Narendra',\n",
       " 'Modi',\n",
       " 'on',\n",
       " 'Friday',\n",
       " ',',\n",
       " 'May',\n",
       " '13',\n",
       " 'expressed',\n",
       " 'condolences',\n",
       " 'over',\n",
       " 'the',\n",
       " 'sudden',\n",
       " 'demise',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'Arab',\n",
       " 'Emirates',\n",
       " 'President',\n",
       " 'Sheikh',\n",
       " 'Khalifa',\n",
       " 'bin',\n",
       " 'Zayed',\n",
       " 'Al',\n",
       " 'Nahyan.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'deeply',\n",
       " 'saddened',\n",
       " 'to',\n",
       " 'know',\n",
       " 'about',\n",
       " 'the',\n",
       " 'passing',\n",
       " 'away',\n",
       " 'of',\n",
       " 'HH',\n",
       " 'Sheikh',\n",
       " 'Khalifa',\n",
       " 'bin',\n",
       " 'Zayed',\n",
       " ',',\n",
       " 'Prime',\n",
       " 'Minister',\n",
       " 'wrote',\n",
       " 'in',\n",
       " 'a',\n",
       " 'twitter',\n",
       " 'post.',\n",
       " 'He',\n",
       " 'reminisced',\n",
       " 'that',\n",
       " 'the',\n",
       " 'late',\n",
       " 'UAE',\n",
       " 'president',\n",
       " 'was',\n",
       " 'a',\n",
       " 'great',\n",
       " 'statesman',\n",
       " 'and',\n",
       " 'visionary',\n",
       " 'leader',\n",
       " 'under',\n",
       " 'whom',\n",
       " 'India-UAE',\n",
       " 'relations',\n",
       " 'prospered.',\n",
       " 'PM',\n",
       " 'Modi',\n",
       " 'sent',\n",
       " 'heartfelt',\n",
       " 'condolences',\n",
       " 'to',\n",
       " 'UAE',\n",
       " 'from',\n",
       " 'the',\n",
       " 'people',\n",
       " 'of',\n",
       " 'India',\n",
       " ',',\n",
       " 'saying',\n",
       " 'that',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'communities',\n",
       " 'were',\n",
       " 'with',\n",
       " 'the',\n",
       " 'people',\n",
       " 'of',\n",
       " 'UAE',\n",
       " 'in',\n",
       " 'these',\n",
       " 'tough',\n",
       " 'times.',\n",
       " 'May',\n",
       " 'his',\n",
       " 'soul',\n",
       " 'rest',\n",
       " 'in',\n",
       " 'peace',\n",
       " ',',\n",
       " 'said',\n",
       " 'Prime',\n",
       " 'Minister',\n",
       " 'Modi',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "  \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461361ef",
   "metadata": {},
   "source": [
    "##### 6. PunktWordTokenizer – It doen’t separates the punctuation from the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68d6d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elon',\n",
       " 'Musk',\n",
       " 'on',\n",
       " 'Friday',\n",
       " 'said',\n",
       " 'that',\n",
       " 'his',\n",
       " 'deal',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'Twitter',\n",
       " 'is',\n",
       " \"'\",\n",
       " 'temporarily',\n",
       " 'on',\n",
       " 'hold',\n",
       " \"'\",\n",
       " 'the',\n",
       " 'social',\n",
       " 'network',\n",
       " 'reported',\n",
       " 'false',\n",
       " 'or',\n",
       " 'spam',\n",
       " 'accounts',\n",
       " 'comprised',\n",
       " 'less',\n",
       " 'than',\n",
       " '5',\n",
       " '%']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "  \n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Elon Musk on Friday said that his deal to buy Twitter is 'temporarily on hold' the social network reported false or spam accounts comprised less than 5%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5dc73",
   "metadata": {},
   "source": [
    "###### 7. Using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4834c8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Russia',\n",
       " 'Ukraine',\n",
       " 'war',\n",
       " 'has',\n",
       " 'entered',\n",
       " 'day',\n",
       " '79',\n",
       " 'with',\n",
       " 'Russia',\n",
       " 'escalating',\n",
       " 'its',\n",
       " 'assault',\n",
       " 'in',\n",
       " 'east',\n",
       " 'and',\n",
       " 'southern',\n",
       " 'Ukraine',\n",
       " 'Meanwhile',\n",
       " 'the',\n",
       " 'US',\n",
       " 'House',\n",
       " 'has',\n",
       " 'passed',\n",
       " 'a',\n",
       " '40bn',\n",
       " 'aid',\n",
       " 'package',\n",
       " 'for',\n",
       " 'Kyiv',\n",
       " 'On',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " 'Russian',\n",
       " 'FM',\n",
       " 'Sergei',\n",
       " 'Lavrov',\n",
       " 'accused',\n",
       " 'the',\n",
       " 'UN',\n",
       " 'of',\n",
       " 'failing',\n",
       " 'to',\n",
       " 'establish',\n",
       " 'a',\n",
       " 'political',\n",
       " 'solution',\n",
       " 'for',\n",
       " 'Ukraine',\n",
       " 'Zelenskyy',\n",
       " 'stated',\n",
       " 'that',\n",
       " 'the',\n",
       " 'war',\n",
       " 'will',\n",
       " 'end',\n",
       " 'for',\n",
       " 'Kyiv',\n",
       " 'only',\n",
       " 'after',\n",
       " 'Russian',\n",
       " 'troops',\n",
       " 'returned',\n",
       " 'all',\n",
       " 'occupied',\n",
       " 'territories']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "  \n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "text = \"The Russia-Ukraine war has entered day 79 with Russia escalating its assault in east and southern Ukraine. Meanwhile, the US House has passed a $40bn aid package for Kyiv. On the other hand, Russian FM Sergei Lavrov accused the UN of failing to establish a political solution for Ukraine. Zelenskyy stated that the war will end for Kyiv only after Russian troops returned all occupied territories.\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029b8b4",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
