{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab91a4ad",
   "metadata": {},
   "source": [
    "### Removing stop words with NLTK in Python\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/32620288/166104650-bca608ed-afc3-4c56-8bf2-eebf0b52b054.png\" width=\"400\" height=\"1\">\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000d135",
   "metadata": {},
   "source": [
    "The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words. \n",
    "\n",
    "What are Stop words?\n",
    "\n",
    "Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. \n",
    "We would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. You can find them in the nltk_data directory. home/pratima/nltk_data/corpora/stopwords is the directory address.(Do not forget to change your home directory name)\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/32620288/166501569-26e7d120-c55b-49b8-9327-0f18a898080b.png\" width=\"400\" height=\"1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f1a3b",
   "metadata": {},
   "source": [
    "#### Removing stop words with NLTK\n",
    "\n",
    "The following program removes stop words from a piece of text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abe1ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#To check the list of stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7555b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fd8ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a key development, German Chancellor Olaf Scholz on Tuesday stated that his country will support the admission of Finland and Sweden to the North Atlantic Treaty Organization (NATO) if they apply for it. He said this while addressing a joint press conference with Finland\\'s Prime Minister Sanna Marin as well as Swedish Prime Minister Sweden Magdalena.  On Tuesday, the Prime Ministers of Finland and Sweden attended a meeting of the German Cabinet in Meseberg. \"It is clear that if these two countries decide to be part of the NATO alliance, they can count on our support,\" Scholz added, as per the DPA news agency.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sent = \"\"\"In a key development, German Chancellor Olaf Scholz on Tuesday stated that his country will support the admission of Finland and Sweden to the North Atlantic Treaty Organization (NATO) if they apply for it. He said this while addressing a joint press conference with Finland's Prime Minister Sanna Marin as well as Swedish Prime Minister Sweden Magdalena.  On Tuesday, the Prime Ministers of Finland and Sweden attended a meeting of the German Cabinet in Meseberg. \"It is clear that if these two countries decide to be part of the NATO alliance, they can count on our support,\" Scholz added, as per the DPA news agency.\"\"\"\n",
    "example_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc8cdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'a', 'key', 'development', ',', 'German', 'Chancellor', 'Olaf', 'Scholz', 'on', 'Tuesday', 'stated', 'that', 'his', 'country', 'will', 'support', 'the', 'admission', 'of', 'Finland', 'and', 'Sweden', 'to', 'the', 'North', 'Atlantic', 'Treaty', 'Organization', '(', 'NATO', ')', 'if', 'they', 'apply', 'for', 'it', '.', 'He', 'said', 'this', 'while', 'addressing', 'a', 'joint', 'press', 'conference', 'with', 'Finland', \"'s\", 'Prime', 'Minister', 'Sanna', 'Marin', 'as', 'well', 'as', 'Swedish', 'Prime', 'Minister', 'Sweden', 'Magdalena', '.', 'On', 'Tuesday', ',', 'the', 'Prime', 'Ministers', 'of', 'Finland', 'and', 'Sweden', 'attended', 'a', 'meeting', 'of', 'the', 'German', 'Cabinet', 'in', 'Meseberg', '.', '``', 'It', 'is', 'clear', 'that', 'if', 'these', 'two', 'countries', 'decide', 'to', 'be', 'part', 'of', 'the', 'NATO', 'alliance', ',', 'they', 'can', 'count', 'on', 'our', 'support', ',', \"''\", 'Scholz', 'added', ',', 'as', 'per', 'the', 'DPA', 'news', 'agency', '.']\n",
      "['In', 'key', 'development', ',', 'German', 'Chancellor', 'Olaf', 'Scholz', 'Tuesday', 'stated', 'country', 'support', 'admission', 'Finland', 'Sweden', 'North', 'Atlantic', 'Treaty', 'Organization', '(', 'NATO', ')', 'apply', '.', 'He', 'said', 'addressing', 'joint', 'press', 'conference', 'Finland', \"'s\", 'Prime', 'Minister', 'Sanna', 'Marin', 'well', 'Swedish', 'Prime', 'Minister', 'Sweden', 'Magdalena', '.', 'On', 'Tuesday', ',', 'Prime', 'Ministers', 'Finland', 'Sweden', 'attended', 'meeting', 'German', 'Cabinet', 'Meseberg', '.', '``', 'It', 'clear', 'two', 'countries', 'decide', 'part', 'NATO', 'alliance', ',', 'count', 'support', ',', \"''\", 'Scholz', 'added', ',', 'per', 'DPA', 'news', 'agency', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "   if w not in stop_words:\n",
    "      filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652c962",
   "metadata": {},
   "source": [
    "#### Performing the Stopwords operations in a file\n",
    "\n",
    "In the code below, text.txt is the original input file in which stopwords are to be removed. filteredtext.txt is the output file. It can be done using following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "453a0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c193722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\divak'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c519cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open('newtext.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "795eb7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Prime Minister Narendra Modi's three-day Europe tour garnered global attention after having struck consecutive business-based meetings with German and Danish leadership and successful interactions with the Indian diaspora in Berlin and Copenhagen. In the latest development pertaining to the premiere's Europe itinerary, PM Modi, as per the Ministry of External Affairs, will make a brief stopover in Paris, given that France currently holds the EU presidency and French President Emmanuel Macron was recently been re-elected for a second consecutive term. Notably, PM Modi will be the first international leader to meet Macron after the latter was re-inducted as the French President.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = file1.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "751eb5fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7d3ee",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
