{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e68594d",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit \n",
    "\n",
    "NLTK is a toolkit build for working with NLP in Python. It provides us various text processing libraries with a lot of test datasets. A variety of tasks can be performed using NLTK such as tokenizing, parse tree visualization, etc… In this article, we will go through how we can set up NLTK in our system and use them for performing various NLP tasks during the text processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b694739",
   "metadata": {},
   "source": [
    "# Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ff5dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ecb4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\divak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dec03bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7764cc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt',\n",
       " '2021-Biden.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beab6c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = inaugural.words('2009-Obama.txt')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96ec5554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2726"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = len(inaugural.words('2009-Obama.txt'))\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55bc9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My fellow citizens : I stand here today humbled by the task before us , grateful for the trust you have bestowed , mindful of the sacrifices borne by our ancestors . I thank President Bush for his service to our nation , as well as the generosity and cooperation he has shown throughout this transition . Forty - four Americans have now taken the presidential oath . The words have been spoken during rising tides of prosperity and the still waters of peace . Yet , every so often the oath is taken amidst gathering clouds and raging storms . At these moments , America has carried on not simply because of the skill or vision of those in high office , but because We the People have remained faithful to the ideals of our forbearers , and true to our founding documents . So it has been . So it must be with this generation of Americans . That we are in the midst of crisis is now well understood . Our nation is at war , against a far - reaching network of violence and hatred . Our economy is badly weakened , a consequence of greed and irresponsibility on the part of some , but also our collective failure to make hard choices and prepare the nation for a new age . Homes have been lost ; jobs shed ; businesses shuttered . Our health care is too costly ; our schools fail too many ; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet . These are the indicators of crisis , subject to data and statistics . Less measurable but no less profound is a sapping of confidence across our land -- a nagging fear that America ' s decline is inevitable , that the next generation must lower its sights . Today I say to you that the challenges we face are real . They are serious and they are many . They will not be met easily or in a short span of time . But know this , America -- they will be met . On this day , we gather because we have chosen hope over fear , unity of purpose over conflict and discord . On this day , we come to proclaim an end to the petty grievances and false promises , the recriminations and worn - out dogmas that for far too long have strangled our politics . We remain a young nation , but in the words of Scripture , the time has come to set aside childish things . The time has come to reaffirm our enduring spirit ; to choose our better history ; to carry forward that precious gift , that noble idea , passed on from generation to generation : the God - given promise that all are equal , all are free , and all deserve a chance to pursue their "
     ]
    }
   ],
   "source": [
    "for word in words[:500]:\n",
    "    print(word, sep =\" \", end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e6207a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\divak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e5f01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5451712c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe8493",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29174c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1495da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\divak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cb98ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Learner! Welcome to NLP (Natural Language Processing) with Python. Here you will learn text mining and processing on natural language data. Are you aware about the Python basics? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_file = open(\"demo_text.txt\",'r')\n",
    "my_text = text_file.read()\n",
    "print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea67ff",
   "metadata": {},
   "source": [
    "## 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7894482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Learner', '!', 'Welcome', 'to', 'NLP', '(', 'Natural', 'Language', 'Processing', ')', 'with', 'Python', '.', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data', '.', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(my_text)\n",
    "print(word_tokens) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78889c",
   "metadata": {},
   "source": [
    "## 2. Sentences Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adbf88d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Learner!', 'Welcome to NLP (Natural Language Processing) with Python.', 'Here you will learn text mining and processing on natural language data.', 'Are you aware about the Python basics?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(my_text)\n",
    "print(sent_tokens) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66667841",
   "metadata": {},
   "source": [
    "## 3. Tokenization (N-Grams)\n",
    "#### Creating Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a806bc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'Learner'), ('Learner', '!'), ('!', 'Welcome'), ('Welcome', 'to'), ('to', 'NLP'), ('NLP', '('), ('(', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing'), ('Processing', ')'), (')', 'with'), ('with', 'Python'), ('Python', '.'), ('.', 'Here'), ('Here', 'you'), ('you', 'will'), ('will', 'learn'), ('learn', 'text'), ('text', 'mining'), ('mining', 'and'), ('and', 'processing'), ('processing', 'on'), ('on', 'natural'), ('natural', 'language'), ('language', 'data'), ('data', '.'), ('.', 'Are'), ('Are', 'you'), ('you', 'aware'), ('aware', 'about'), ('about', 'the'), ('the', 'Python'), ('Python', 'basics'), ('basics', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "my_words = word_tokenize(my_text) # This is the list of all words\n",
    "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\n",
    "print(twograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1727cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'Learner'), ('Learner', '!'), ('!', 'Welcome'), ('Welcome', 'to'), ('to', 'NLP'), ('NLP', '('), ('(', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing'), ('Processing', ')'), (')', 'with'), ('with', 'Python'), ('Python', '.'), ('.', 'Here'), ('Here', 'you'), ('you', 'will'), ('will', 'learn'), ('learn', 'text'), ('text', 'mining'), ('mining', 'and'), ('and', 'processing'), ('processing', 'on'), ('on', 'natural'), ('natural', 'language'), ('language', 'data'), ('data', '.'), ('.', 'Are'), ('Are', 'you'), ('you', 'aware'), ('aware', 'about'), ('about', 'the'), ('the', 'Python'), ('Python', 'basics'), ('basics', '?')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(my_words))\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a00f34",
   "metadata": {},
   "source": [
    "### Creating trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ad3c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'Learner', '!'), ('Learner', '!', 'Welcome'), ('!', 'Welcome', 'to'), ('Welcome', 'to', 'NLP'), ('to', 'NLP', '('), ('NLP', '(', 'Natural'), ('(', 'Natural', 'Language'), ('Natural', 'Language', 'Processing'), ('Language', 'Processing', ')'), ('Processing', ')', 'with'), (')', 'with', 'Python'), ('with', 'Python', '.'), ('Python', '.', 'Here'), ('.', 'Here', 'you'), ('Here', 'you', 'will'), ('you', 'will', 'learn'), ('will', 'learn', 'text'), ('learn', 'text', 'mining'), ('text', 'mining', 'and'), ('mining', 'and', 'processing'), ('and', 'processing', 'on'), ('processing', 'on', 'natural'), ('on', 'natural', 'language'), ('natural', 'language', 'data'), ('language', 'data', '.'), ('data', '.', 'Are'), ('.', 'Are', 'you'), ('Are', 'you', 'aware'), ('you', 'aware', 'about'), ('aware', 'about', 'the'), ('about', 'the', 'Python'), ('the', 'Python', 'basics'), ('Python', 'basics', '?')]\n"
     ]
    }
   ],
   "source": [
    "threegrams = list(ngrams(my_words,3))\n",
    "print(threegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eab3cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'Learner', '!'), ('Learner', '!', 'Welcome'), ('!', 'Welcome', 'to'), ('Welcome', 'to', 'NLP'), ('to', 'NLP', '('), ('NLP', '(', 'Natural'), ('(', 'Natural', 'Language'), ('Natural', 'Language', 'Processing'), ('Language', 'Processing', ')'), ('Processing', ')', 'with'), (')', 'with', 'Python'), ('with', 'Python', '.'), ('Python', '.', 'Here'), ('.', 'Here', 'you'), ('Here', 'you', 'will'), ('you', 'will', 'learn'), ('will', 'learn', 'text'), ('learn', 'text', 'mining'), ('text', 'mining', 'and'), ('mining', 'and', 'processing'), ('and', 'processing', 'on'), ('processing', 'on', 'natural'), ('on', 'natural', 'language'), ('natural', 'language', 'data'), ('language', 'data', '.'), ('data', '.', 'Are'), ('.', 'Are', 'you'), ('Are', 'you', 'aware'), ('you', 'aware', 'about'), ('aware', 'about', 'the'), ('about', 'the', 'Python'), ('the', 'Python', 'basics'), ('Python', 'basics', '?')]\n"
     ]
    }
   ],
   "source": [
    "trigrams = list(nltk.trigrams(my_words))\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2173ee",
   "metadata": {},
   "source": [
    "# 1. Tokenization using Python’s split() function\n",
    "Let’s start with the split() method as it is the most basic one. It returns a list of strings after breaking the given string by the specified separator. By default, split() breaks a string at each space. We can change the separator to anything. Let’s check it out.\n",
    "\n",
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e121516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Learner!', 'Welcome', 'to', 'NLP', '(Natural', 'Language', 'Processing)', 'with', 'Python.', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data.', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics?']\n"
     ]
    }
   ],
   "source": [
    "split_word_tokens = my_text.split()\n",
    "print(split_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de05155",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cbece46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Learner! Welcome to NLP (Natural Language Processing) with Python', 'Here you will learn text mining and processing on natural language data', 'Are you aware about the Python basics? \\n']\n"
     ]
    }
   ],
   "source": [
    "split_sent_tokens = my_text.split('. ')\n",
    "print(split_sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4a4b5",
   "metadata": {},
   "source": [
    "# 2. Tokenization using Regular Expressions (RegEx)\n",
    "First, let’s understand what a regular expression is. It is basically a special character sequence that helps you match or find other strings or sets of strings using that sequence as a pattern.\n",
    "\n",
    "We can use the re library in Python to work with regular expression. This library comes preinstalled with the Python installation package.\n",
    "\n",
    "Now, let’s perform word tokenization and sentence tokenization keeping RegEx in mind.\n",
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eab7f506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Learner!', 'Welcome', 'to', 'NLP', '(Natural', 'Language', 'Processing)', 'with', 'Python.', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data.', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# RegEx Tokenizer with whitespace delimiter\n",
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps = True)\n",
    "\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(my_text)\n",
    "print(whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1ff52bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Learner', 'Welcome', 'to', 'NLP', 'Natural', 'Language', 'Processing', 'with', 'Python', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics']\n"
     ]
    }
   ],
   "source": [
    "re_word_tokens = re.findall(\"[\\w']+\", my_text)\n",
    "print(re_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bba1f0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Learner', 'Welcome', 'NLP', 'Natural', 'Language', 'Processing', 'Python', 'Here', 'Are', 'Python']\n"
     ]
    }
   ],
   "source": [
    "# RegexpTokenizer to match only capitalized words\n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "print(cap_tokenizer.tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6d116",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a95fb957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Learner', 'Welcome to NLP (Natural Language Processing) with Python', 'Here you will learn text mining and processing on natural language data', 'Are you aware about the Python basics', '\\n']\n"
     ]
    }
   ],
   "source": [
    "re_sentence_tokens = re.compile('[.!?] ').split(my_text)\n",
    "print(re_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf53c59",
   "metadata": {},
   "source": [
    "# 3. Tokenization using NLTK\n",
    "NLTK contains a module called tokenize() which further classifies into two sub-categories:\n",
    "\n",
    "Word tokenize: We use the word_tokenize() method to split a sentence into tokens or words Sentence tokenize: We use the sent_tokenize() method to split a document or paragraph into sentences\n",
    "\n",
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7540fd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Learner', '!', 'Welcome', 'to', 'NLP', '(', 'Natural', 'Language', 'Processing', ')', 'with', 'Python', '.', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data', '.', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_word_tokens = word_tokenize(my_text)\n",
    "print(nltk_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f474949",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afe2afca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Learner!', 'Welcome to NLP (Natural Language Processing) with Python.', 'Here you will learn text mining and processing on natural language data.', 'Are you aware about the Python basics?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk_sent_tokens = sent_tokenize(my_text)\n",
    "print(nltk_sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097685d",
   "metadata": {},
   "source": [
    "# 4. Tokenization using Keras\n",
    "Keras! One of the hottest deep learning frameworks in the industry right now. It is an open-source neural network library for Python. Keras is super easy to use and can also run on top of TensorFlow.\n",
    "\n",
    "In the NLP context, we can use Keras for cleaning the unstructured text data that we typically collect.\n",
    "\n",
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d714d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ec1168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'learner', 'welcome', 'to', 'nlp', 'natural', 'language', 'processing', 'with', 'python', 'here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data', 'are', 'you', 'aware', 'about', 'the', 'python', 'basics']\n"
     ]
    }
   ],
   "source": [
    "keras_word_tokens = text_to_word_sequence(my_text)\n",
    "print(keras_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995cc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
